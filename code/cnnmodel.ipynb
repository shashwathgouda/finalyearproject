{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries!","metadata":{"_uuid":"c5fde02a-99c6-45f1-a39b-457e3d15cdaf","_cell_guid":"b4108fbc-e69e-4683-b54c-8766251150a3","trusted":true}},{"cell_type":"code","source":"import numpy as np #Importing Numpy for Mathematical Operations.\n\nimport pandas as pd #Importing Pandas for Data Analysis.\n\nfrom pathlib import Path #The Pathlib Module in Python simplifies the way in working with Files and Folders.\n\n                         #The Path is used to Identify a File. The Path provides an optional sequence of directory \n                         #names terminated by the Final File Name including the Filename Extension.\n        \nimport os.path #This Module implements some useful functions on Pathnames.\n\nimport matplotlib.pyplot as plt #Importing for Plotting. \n\nfrom IPython.display import Image, display, Markdown #Certain Display Functionalities.\n\nimport matplotlib.cm as cm #To modify the ColorMaps.\n\nfrom sklearn.model_selection import train_test_split #Sklearn model selection for splitting data arrays into two subsets: \n                                                     #For Training Data and for Testing Data. With this function,\n                                                     #you don't need to divide the dataset manually. By default, \n                                                     #Sklearn train_test_split will make random partitions for the two subsets.\n            \nfrom sklearn.metrics import confusion_matrix #For Confusion Matrix\n\nimport tensorflow as tf\nfrom time import perf_counter #Time module provides various time-related functions i.e. to show time.\n\nimport seaborn as sns #Data visualization library built on top of Matplotlib.\n\ndef printmd(string):\n    # Print with Markdowns:  \n    display(Markdown(string))","metadata":{"_uuid":"2b6d78ad-1849-405b-b63e-056ee81198b0","_cell_guid":"889319eb-82f4-4126-91b8-92b28f9c3157","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T15:25:45.268935Z","iopub.execute_input":"2022-05-09T15:25:45.269205Z","iopub.status.idle":"2022-05-09T15:25:45.275955Z","shell.execute_reply.started":"2022-05-09T15:25:45.269176Z","shell.execute_reply":"2022-05-09T15:25:45.275217Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_dir = Path('../input/cancer-dataset/Dataset')\n\n# Get filepaths and labels:\nfilepaths = list(image_dir.glob(r'**/*.png')) \n\n\"\"\"glob is a powerful tool in Python to help with file management and filtering. \nWhile os helps to manage and create specific paths that are friendly to whatever machine they are used on, \nglob helps to filter through large datasets and pull out only files that are of interest.\n\nThe * is a sort of wildcard that can be used to search for items that have differences in their names. \nWhatever text doesn’t match can be replaced by a *.\nFor example, if you want every file in a directory to be returned to you, you can put a * at the end of a directory path.\nglob will return a list of all of the files in that directory.\"\"\"\n\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths)) #0 denotes head and 1 denotes tail i.e.\n\n#os.path.split() method in Python is used to Split the path name into a pair of Head and Tail. \n#Here, Tail is the last path name component and Head is everything leading up to that.\n\n#For example consider the following path name:\n#path name = '/home/User/Desktop/file.txt'\n#In the above example ‘file.txt’ component of path name is Tail and ‘/home/User/Desktop/’ is Head.\n#The tail part will never contain a slash; if name of the path ends with a slash, tail will be empty \n#and if there is no slash in path name, head will be empty.","metadata":{"_uuid":"87bdea30-36c0-439e-91d1-fdcd5aa4577a","_cell_guid":"c4b4b3b4-47b7-4444-9d48-27d4b7ff8a15","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T15:26:10.292539Z","iopub.execute_input":"2022-05-09T15:26:10.292803Z","iopub.status.idle":"2022-05-09T15:26:11.110727Z","shell.execute_reply.started":"2022-05-09T15:26:10.292776Z","shell.execute_reply":"2022-05-09T15:26:11.109896Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generating Series of Data:\nfilepaths = pd.Series(filepaths, name='Filepath').astype(str) \nlabels = pd.Series(labels, name='Label')\n\n#Concatenate filepaths and labels:\nimage_df = pd.concat([filepaths, labels], axis=1) \n\n#axis: It specifies the axis along which any values are computed. By default axis=0\n# Axis 0 will act on all the ROWS in each COLUMN\n# Axis 1 will act on all the COLUMNS in each ROW\n\n\"\"\"\n+------------+---------+--------+\n|            |  A      |  B     |\n+------------+---------+---------\n|      0     | 0.626386| 1.52325|----axis=1----->\n+------------+---------+--------+\n             |         |\n             | axis=0  |\n             ↓         ↓                \n\"\"\"\n\n#Shuffle the DataFrame and Reset Index:\nimage_df = image_df.sample(frac=1).reset_index(drop = True)\n\n#The \".sample\": to sample all rows without replacement of a dataframe.\n\n#The frac keyword argument specifies the fraction of rows to return in the random sample, \n#so \"frac=1\" means return all rows (in random order).\n\n#Here, specifying \"drop=True\" prevents \".reset_index\" from creating a column containing the old index entries.\n\n#Show the result:\nimage_df.head(5)","metadata":{"_uuid":"32ac2d1d-ebfd-4bfd-9903-021845fe5d7d","_cell_guid":"662feb02-2d51-43af-b95d-f4291bf69847","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T15:28:16.698046Z","iopub.execute_input":"2022-05-09T15:28:16.698336Z","iopub.status.idle":"2022-05-09T15:28:16.715794Z","shell.execute_reply.started":"2022-05-09T15:28:16.698305Z","shell.execute_reply":"2022-05-09T15:28:16.715029Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization!","metadata":{"_uuid":"962065a9-4110-45ec-ad93-520ca1d7e3e0","_cell_guid":"000b1414-cc3f-446f-89c5-4c463f429009","trusted":true}},{"cell_type":"code","source":"#Display some pictures of the dataset with their labels:\n\nfig, axes = plt.subplots(nrows=3, ncols=4, figsize=(20, 15),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\n\"\"\"The matplotlib.pyplot.subplots method provides a way to plot multiple plots on a single figure. \nGiven the number of rows and columns, it returns a tuple (fig, ax), giving a single figure \"fig\" with an array of axes \"ax\".\"\"\"\n\n\n\"\"\"1. nrows, ncols: Number of rows/columns of the subplot grid.\n   2. subplot_kw: Dict of keywords to be passed to the add_subplot call to add keywords to each subplot. \n   The default value is None.\n   3. Ticks are the markers denoting data points on axes, The xticks() and yticks() function takes a list object as argument. \n   The elements in the list denote the positions on corresponding action where ticks will be displayed.\"\"\"\n   \nfor i, ax in enumerate(axes.flat): #axes.flat: For each iteration it would yield the next axes from that array, such \n                                              #that you may easily plot to all axes in a single loop.\n                                #axes.flat is not a function, it's an attribute of the numpy.ndarray: numpy.ndarray.flat\n                               #ndarray.flat: A 1-D iterator over the array.\n        \n        \n    ax.imshow(plt.imread(image_df.Filepath[i])) #Displaying Image.\n    \n    ax.set_title(image_df.Label[i]) #Displaying Lable i.e. title.\n    \nplt.tight_layout() #tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area.\n\nplt.show()","metadata":{"_uuid":"76ab88eb-5ad1-47b5-b257-24ea57e87e13","_cell_guid":"b5545e5a-b69a-453f-9142-3abc072fc79f","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T15:32:22.255714Z","iopub.execute_input":"2022-05-09T15:32:22.256434Z","iopub.status.idle":"2022-05-09T15:32:23.309588Z","shell.execute_reply.started":"2022-05-09T15:32:22.256381Z","shell.execute_reply":"2022-05-09T15:32:23.308848Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the number of pictures of each category:\n\nvc = image_df['Label'].value_counts() #Calling the value_counts method to get a count of unique values.\nplt.figure(figsize=(20,15))\n\nsns.barplot(x = vc.index, y = vc, palette = \"rocket\") \n#One of the color palette \"rocket\" is used to display graph.\n#x-axis: index value i.e. label names.\n#y-axis: value count.\n\nplt.title(\"Number of pictures of each category\", fontsize = 11)\nplt.show()","metadata":{"_uuid":"7a94bece-942d-4729-b4bd-29594e36a192","_cell_guid":"ddd628c6-9d13-4f77-895e-02e3eac3dd4c","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T15:38:29.675666Z","iopub.execute_input":"2022-05-09T15:38:29.675941Z","iopub.status.idle":"2022-05-09T15:38:30.066048Z","shell.execute_reply.started":"2022-05-09T15:38:29.675911Z","shell.execute_reply":"2022-05-09T15:38:30.065369Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the Images with a generator!","metadata":{"_uuid":"e9509061-389c-4405-bbb9-f4b1d1339528","_cell_guid":"336550cb-6545-4849-ba57-9c187b001570","trusted":true}},{"cell_type":"code","source":"def create_gen():\n    \n    #Load the Images with a generator and Data Augmentation:\n    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n        validation_split=0.2\n    )\n\n    test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input)\n    \n        #Pre-Processes a tensor or Numpy array encoding a batch of images.\n        #Returns Pre-Processed numpy.array or a tf.Tensor with type float32.\n        #The inputs pixel values are scaled between -1 and 1, sample-wise.\n    \n\n    train_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb', #One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n                          #Whether the images will be converted to have 1, 3, or 4 channels.\n        \n                          #rgba(red, green, blue, alpha)\n                          #The alpha value is declared as a decimal number from 0 to 1, where 0 is fully \n                          #transparent and 1 is fully opaque.\n                          \n        \n        \n        class_mode='categorical',   #categorical\": 2D output (aka. list of numbers of length N), [0, 0, 1, 0], \n                                    #which is a one-hot encoding (only one number is 1/ \"hot\") representing the donkey. \n                                    #This is for mutually exclusive labels. A dog cannot be a cat, a human is not a dog.\n        \n        batch_size=16, #Size of the batches of data - Default: 32.\n        \n        shuffle=True, #Whether to shuffle the data. Default: True. If set to False, sorts the data in alphanumeric order.\n        \n        seed=0, #Optional random seed for shuffling and transformations.\n        \n        subset='training',  #One of \"training\" or \"validation\". Only used if validation_split is set.\n        \n        rotation_range=30,  #rotation_range is a value in degrees (0-180), a range within which to randomly rotate pictures.\n        \n        zoom_range=0.15, #zoom_range is for randomly zooming inside pictures.\n        \n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        \n        #width_shift and height_shift are ranges (as a fraction of total width or height) \n        #within which to randomly translate pictures vertically or horizontally.\n        \n#With width_shift_range=2 possible values are integers [-1, 0, +1], same as with width_shift_range=[-1, 0, +1], \n#while with width_shift_range=1.0 possible values are floats in the interval [-1.0, +1.0).\"\"\"\n        \n#With height_shift_range=2 possible values are integers [-1, 0, +1], same as with height_shift_range=[-1, 0, +1],\n#while with height_shift_range=1.0 possible values are floats in the interval [-1.0, +1.0).\"\"\"\n        \n        \n        \n        \n        shear_range=0.15, #shear_range is for randomly applying shearing transformations.\n        \n#In plane geometry, a shear mapping is a linear map that displaces each point in a fixed direction, \n#by an amount proportional to its signed distance from the line that is parallel to that direction and \n#goes through the origin. This type of mapping is also called shear transformation, transvection, or just shearing.\n        \n        horizontal_flip=True, #horizontal_flip is for randomly flipping half of the images horizontally \n                              #--relevant when there are no assumptions of horizontal assymetry (e.g. real-world pictures).\n        \n        fill_mode=\"nearest\")\n    \n        #fill_mode is the strategy used for filling in newly created pixels, \n        #which can appear after a rotation or a width/height shift.\n        \n#         One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}. Default is 'nearest'. \n#         Points outside the boundaries of the input are filled according to the given mode: - \n#         'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k) - \n#         'nearest': aaaaaaaa|abcd|dddddddd - \n#         'reflect': abcddcba|abcd|dcbaabcd - \n#         'wrap': abcdabcd|abcd|abcdabcd\n  \n    \n#    The flow_from_directory() method allows you to read the images directly \n#    from the directory and augment them while the neural network model is learning on the training data.\n#    The method expects that images belonging to different classes are present in different folders but \n#    are inside the same parent folder.\n\n#     By doing this, we are instructing our data generator to apply all \n#     function to every image as a preprocessing step before feeding it to the model. \n#     This way, we eliminate the need to process all the images and write them to a separate directory. \n    \n    \n    \n    val_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=16,\n        shuffle=True,\n        seed=0,\n        subset='validation',\n        rotation_range=30, \n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        horizontal_flip=True,\n        fill_mode=\"nearest\"\n    )\n\n    test_images = test_generator.flow_from_dataframe(\n        dataframe=test_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=16,\n        shuffle=False\n    )\n    \n    return train_generator,test_generator,train_images,val_images,test_images","metadata":{"_uuid":"5a57b45e-d421-4239-8ced-921782ac80c7","_cell_guid":"8a261bd8-05e0-4925-85d4-1e3ffc872a14","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T15:58:41.415668Z","iopub.execute_input":"2022-05-09T15:58:41.415935Z","iopub.status.idle":"2022-05-09T15:58:41.430852Z","shell.execute_reply.started":"2022-05-09T15:58:41.415907Z","shell.execute_reply":"2022-05-09T15:58:41.430160Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Separate in Train and Test Data:\ntrain_df, test_df = train_test_split(image_df, train_size=0.7, shuffle=True, random_state=1)\n\n#The random_state parameter is used for initializing the internal random number generator,\n#which will decide the splitting of data into train and test indices.\n\n# Setting random_state a fixed value will guarantee that the same sequence of random numbers \n# is generated each time you run the code.","metadata":{"_uuid":"e77af9c6-ee1b-4a86-8499-2c9526eb9112","_cell_guid":"ce81fa9e-84f6-41c7-ae1b-2b66752b669e","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T16:00:11.391713Z","iopub.execute_input":"2022-05-09T16:00:11.392282Z","iopub.status.idle":"2022-05-09T16:00:11.401565Z","shell.execute_reply.started":"2022-05-09T16:00:11.392246Z","shell.execute_reply":"2022-05-09T16:00:11.400743Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the Generators:\ntrain_generator,test_generator,train_images,val_images,test_images = create_gen()\nprint('\\n')","metadata":{"_uuid":"eb93ddb2-439c-4a23-a869-7157ff24d34f","_cell_guid":"33d273c6-c49c-4ec0-8b42-5b1e8157e6f3","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T16:00:13.990958Z","iopub.execute_input":"2022-05-09T16:00:13.991249Z","iopub.status.idle":"2022-05-09T16:00:18.765582Z","shell.execute_reply.started":"2022-05-09T16:00:13.991215Z","shell.execute_reply":"2022-05-09T16:00:18.764101Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers #Importing layers from Keras as it \n                                    #lets you create a model layer by layer for most problems. \n    \n!pip install tensorflow-addons==0.9.1\nimport tensorflow_addons\n\n# TensorFlow Addons is a repository of contributions that conform \n# to well-established API patterns, but implement new functionality not available in core TensorFlow. \n# TensorFlow natively supports a large number of operators, layers, metrics, losses, and optimizers.\n# However, in a fast moving field like ML, there are many interesting new developments that cannot be \n# integrated into core TensorFlow (because their broad applicability is not yet clear, or it is mostly \n# used by a smaller subset of the community).\n\nfrom tensorflow_addons.metrics import F1Score, CohenKappa #Using more Metrics.","metadata":{"_uuid":"3bb24627-239d-474f-94b6-75f871e6b827","_cell_guid":"5ccb7e95-9fd1-4b1b-98e5-20ff1f50cfc4","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T16:01:52.523722Z","iopub.execute_input":"2022-05-09T16:01:52.524016Z","iopub.status.idle":"2022-05-09T16:02:13.646134Z","shell.execute_reply.started":"2022-05-09T16:01:52.523985Z","shell.execute_reply":"2022-05-09T16:02:13.645347Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    layers.Conv2D(16, (3,3), padding=\"same\", input_shape=(224,224,3), activation = 'relu'),\n    layers.MaxPooling2D(pool_size=(2,2)),\n    layers.BatchNormalization(),\n    \n#    1. The first required Conv2D parameter is the number of filters that the convolutional layer will learn.\n\n#    2. We learn a total of 16 filters. Max pooling is then used to reduce the dimensions of the output volume.\n   \n#    Note: Notice, as our output volume is decreasing our number of filters learned is increasing — \n#    this is a common practice in designing CNN architectures\n   \n#    3. Now after filter_layers we provide kernel_size. Typical values for kernel_size include: (1, 1) , (3, 3) , (5, 5) , (7, 7). \n#    It’s rare to see kernel sizes larger than 7×7. It's recommended that when we have Input images greater then 128x128,\n#    we should use kernel size >=3, to help (1) learn larger filters and (2) to help reduce volume size.\n   \n#    4. Padding: If you instead want to preserve the dimensions of the volume such that the output volume size \n#    matches the input volume size, then you would want to supply a value of \"same\" for the padding.\n#    With the \"valid\" parameter the input volume is not zero-padded and the dimensions are \n#    allowed to reduce via the natural application of convolution.\n    \n    layers.Conv2D(32, (3,3), padding=\"same\", activation = 'relu'),\n    layers.MaxPooling2D(pool_size=(2,2)),\n    layers.BatchNormalization(),\n \n    layers.Conv2D(64, (3,3), padding=\"same\", activation = 'relu'),\n    layers.MaxPooling2D(pool_size=(2,2)),\n    layers.BatchNormalization(),\n    \n    layers.Conv2D(64, (3,3), padding=\"same\", activation = 'relu'),\n    layers.MaxPooling2D(pool_size=(2,2)),\n    layers.BatchNormalization(),\n    \n    layers.Conv2D(128, (3,3), padding=\"same\", activation = 'relu'),\n    layers.MaxPooling2D(pool_size=(2,2)),\n    layers.BatchNormalization(),\n    \n    layers.Conv2D(128, (3,3), padding=\"same\", activation = 'relu'),\n    layers.MaxPooling2D(pool_size=(2,2)),\n    layers.BatchNormalization(),\n    \n    layers.Conv2D(256, (3,3), padding=\"same\", activation = 'relu'),\n    layers.MaxPooling2D(pool_size=(2,2)),\n    layers.BatchNormalization(),\n    \n    layers.Conv2D(256, (3,3), padding=\"same\", activation = 'relu'),\n    layers.MaxPooling2D(pool_size=(1,1)),\n    layers.BatchNormalization(),\n    \n    layers.Flatten(), #To bring all levels of a multi-layered image down to one plane i.e. used to get a copy of a given \n                      #array collapsed into one dimension.\n    \n    #1st FC Layer:\n    layers.Dense(33, activation = 'relu'), \n    #layers.Dense(): This function is used to create fully connected layers, \n    #in which every output depends on every input.\n    \n    layers.Dropout(0.15),\n    \n    #Dropout refers to ignoring units (i.e. neurons) during the training phase of certain set \n    #of neurons which is chosen at random. \n    #By “ignoring”, means these units are not considered during a particular forward or backward pass.\n    #Use: To prevent over-fitting.\n    \n    #2nd FC Layer:\n    layers.Dense(33, activation = 'softmax')\n])\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy','AUC']\n)\n\n#       Optimizers are Classes or methods used to change the attributes of your machine/deep learning model \n#       such as weights and learning rate in order to reduce the losses. Optimizers help to get results faster.\n#       Adam stands for Adaptive Moment Estimation, which is another way of using past gradients to calculate \n#       current gradients.\n#       Adam utilizes the concept of momentum (Momentum can smooth the progression of the learning \n#       algorithm that, in turn, can accelerate the training process), by adding fractions of previous gradients \n#       to the current \n#       one, it is practically accepted in many projects during training neural nets.\n\n\n\nmodel.summary()\n\n# param_number = output_channel_number * (input_channel_number * kernel_height * kernel_width + 1)","metadata":{"_uuid":"5b398c00-1766-48b9-8287-e2391c470dfa","_cell_guid":"0b13bac4-ee2c-41b7-88c0-3873ef39508a","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T16:24:14.550556Z","iopub.execute_input":"2022-05-09T16:24:14.550919Z","iopub.status.idle":"2022-05-09T16:24:17.541890Z","shell.execute_reply.started":"2022-05-09T16:24:14.550882Z","shell.execute_reply":"2022-05-09T16:24:17.540877Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_images,\n                    epochs=50,                   # No. of Iterations.\n                    validation_data = val_images)","metadata":{"_uuid":"9cf90137-7254-4cf0-88f4-845d4194afd2","_cell_guid":"8187c91d-a52a-4fba-b02d-31998c1dc8b9","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T16:37:29.535322Z","iopub.execute_input":"2022-05-09T16:37:29.535623Z","iopub.status.idle":"2022-05-09T16:44:21.218190Z","shell.execute_reply.started":"2022-05-09T16:37:29.535591Z","shell.execute_reply":"2022-05-09T16:44:21.217326Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\nplt.title(\"Accuracy\")\nplt.show()\n\npd.DataFrame(history.history)[['loss','val_loss']].plot()\nplt.title(\"Loss\")\nplt.show()","metadata":{"_uuid":"718e476c-53bb-4c99-b2ee-610de09829de","_cell_guid":"2fbb60ec-d541-4942-918d-68d1dbf23c0d","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T16:49:38.446156Z","iopub.execute_input":"2022-05-09T16:49:38.446452Z","iopub.status.idle":"2022-05-09T16:49:38.900182Z","shell.execute_reply.started":"2022-05-09T16:49:38.446414Z","shell.execute_reply":"2022-05-09T16:49:38.899300Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = model.evaluate(test_images, verbose=0)\n\"\"\"By setting verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch.\nverbose=0 will show you nothing (silent)\nverbose=1 will show you an animated progress bar like this:\n[==============================]\nverbose=2 will just mention the number of epoch like this:\nEpoch 1/50 \"\"\"","metadata":{"_uuid":"a30dd877-1d44-4c81-a5a6-7f559fbc8a56","_cell_guid":"a12eb006-bf28-427e-afeb-c6275358bf0f","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T16:49:42.235267Z","iopub.execute_input":"2022-05-09T16:49:42.235745Z","iopub.status.idle":"2022-05-09T16:49:49.468592Z","shell.execute_reply.started":"2022-05-09T16:49:42.235705Z","shell.execute_reply":"2022-05-09T16:49:49.467915Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"printmd(\" ## Test Loss: {:.5f}\".format(results[0]))\nprintmd(\"## Accuracy on the test set: {:.2f}%\".format(results[1] * 100))\nprint('\\n')","metadata":{"_uuid":"c7fd2c4d-b4e3-4c35-a979-81c62d70c736","_cell_guid":"38a03b3a-a6f0-43e9-a5fd-a5e833714121","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T16:49:52.742065Z","iopub.execute_input":"2022-05-09T16:49:52.742325Z","iopub.status.idle":"2022-05-09T16:49:52.751817Z","shell.execute_reply.started":"2022-05-09T16:49:52.742295Z","shell.execute_reply":"2022-05-09T16:49:52.750959Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict the label of the test_images:\npred = model.predict(test_images)\npred = np.argmax(pred,axis=1)\n\"\"\"axis: It specifies the axis along which the any values are computed. By default axis=0\n1. Axis 0 will act on all the ROWS in each COLUMN\n2. Axis 1 will act on all the COLUMNS in each ROW \"\"\"\n\n# Map the label:\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items()) #v=value, k=key in dictionary.\npred = [labels[k] for k in pred]\n\n# Display the result:\nprint(f'The first 5 predictions: {pred[:5]}')","metadata":{"_uuid":"ad433a5c-3fee-4b46-a55b-bf2cc1bbe5c5","_cell_guid":"ce953a16-7426-4150-84d4-2d98019337e3","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T16:50:13.637453Z","iopub.execute_input":"2022-05-09T16:50:13.638210Z","iopub.status.idle":"2022-05-09T16:50:17.253951Z","shell.execute_reply.started":"2022-05-09T16:50:13.638165Z","shell.execute_reply":"2022-05-09T16:50:17.253078Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ny_test = list(test_df.Label)\nfrom sklearn import metrics\n\n#Calculating metrics and rounding to 5 decimals:\nprint('Accuracy:', np.round(metrics.accuracy_score(y_test,pred),5)) \nprint('Precision:', np.round(metrics.precision_score(y_test,pred, average='weighted'),5))\n\n\"\"\" Weighted average is a calculation that takes into account the varying degrees of importance of the numbers in a data set.\n    In calculating a weighted average, each number in the data set is multiplied by a predetermined weight before the final \n    calculation is made.\n    A weighted average can be more accurate than a simple average in which all numbers in a data set are assigned an \n    identical weight. \"\"\"\n\nprint('Recall:', np.round(metrics.recall_score(y_test,pred, average='weighted'),5))\nprint('F1 Score:', np.round(metrics.f1_score(y_test,pred, average='weighted'),5))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test,pred),5))\nprint(classification_report(y_test, pred))","metadata":{"_uuid":"c90b8fac-6591-47d5-ae72-dd8298da584f","_cell_guid":"111b1445-2618-4824-a7c4-4925b6ae3f99","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T16:51:09.456455Z","iopub.execute_input":"2022-05-09T16:51:09.457168Z","iopub.status.idle":"2022-05-09T16:51:09.509028Z","shell.execute_reply.started":"2022-05-09T16:51:09.457130Z","shell.execute_reply":"2022-05-09T16:51:09.508247Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cf_matrix = confusion_matrix(y_test, pred, normalize='true')\n\n\"\"\"normalize{‘true’, ‘pred’, ‘all’}, default=None\nNormalizes confusion matrix over the true (rows), predicted (columns)\nconditions or all the population. If None, confusion matrix will not be normalized.\"\"\"\n\nplt.figure(figsize = (25,20))\nsns.heatmap(cf_matrix, annot=True, xticklabels = sorted(set(y_test)), yticklabels = sorted(set(y_test)))\n\n#annot: If True, write the data value in each cell.\n\n\n\nplt.title('Normalized Confusion Matrix')\nplt.show()","metadata":{"_uuid":"04505c18-0f81-45ed-ae11-4bfdcd868f5e","_cell_guid":"48c113b4-181b-43b5-98c4-94a4f6d1be8a","collapsed":false,"execution":{"iopub.status.busy":"2022-05-09T16:51:12.715322Z","iopub.execute_input":"2022-05-09T16:51:12.716259Z","iopub.status.idle":"2022-05-09T16:51:17.450011Z","shell.execute_reply.started":"2022-05-09T16:51:12.716196Z","shell.execute_reply":"2022-05-09T16:51:17.447140Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"8793158d-5085-4e98-a588-9bb73e1a1399","_cell_guid":"5e9ef8b9-f946-4457-9a5c-23921ff27894","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}